{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd  \n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb613d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_id=\"HalcrowLithology\"\n",
    "\n",
    "# Set the environment variables\n",
    "os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"] = original_endpoint\n",
    "os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"] = original_key\n",
    "model_id = os.getenv(\"CUSTOM_BUILT_MODEL_ID\", custom_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your `DocumentAnalysisClient` instance and `AzureKeyCredential` variable\n",
    "document_analysis_client = DocumentAnalysisClient(endpoint=\"AZURE_FORM_RECOGNIZER_ENDPOINT\", credential=AzureKeyCredential(\"AZURE_FORM_RECOGNIZER_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ff8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Folder Content\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "model_id = os.getenv(\"CUSTOM_BUILT_MODEL_ID\", custom_model_id)\n",
    "dpath=r\"C:\\Users\\test\\Data\\Lithology\\Halcraw1969_PT_Lithology\\Training\"\n",
    "\n",
    "i=85\n",
    "columns = ['Well Name', 'Easting', 'Northing', 'Location', 'Yield', 'Drawdown', 'Conductivity', 'GWL', 'Depth', 'Drill Date','Page']\n",
    "dataset = pd.DataFrame(columns=columns)\n",
    "\n",
    "# specify the path to the folder containing the PDF files\n",
    "pdf_folder = os.path.join(dpath, \"Dataset\")\n",
    "\n",
    "# get a list of file paths in the folder\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "# iterate through the files and process them one by one\n",
    "for path_to_sample_documents in pdf_files:\n",
    "    # add your code here to process the PDF file\n",
    "#     print(\"Processing file:\", path_to_sample_documents)\n",
    "\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    # Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "    with open(path_to_sample_documents, \"rb\") as f:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            model_id=model_id, document=f\n",
    "        )\n",
    "    result = poller.result()\n",
    "    # Extracting Well Details\n",
    "    row = []\n",
    "    data=result.documents[0].to_dict()['fields']\n",
    "    for col in columns:\n",
    "        try:\n",
    "            row.append(data.get(col)['value'])\n",
    "        except Exception as e:\n",
    "            print(f\"An exception occurred while processing: {d}\")\n",
    "            print(e)\n",
    "    row = pd.DataFrame([row],columns=columns)\n",
    "    dataset = pd.concat([dataset, row], ignore_index=True) \n",
    "    dataset.to_csv(os.path.join(dpath, \"output\\WellsDataset.csv\"), index=False)\n",
    "    \n",
    "    #Extracting Lithology Logs\n",
    "    data2=result.documents[0].to_dict()['fields']['Lithology']['value']\n",
    "    rows = []\n",
    "    for d in data2:\n",
    "        try:\n",
    "            from_val = d['value']['From']['content']\n",
    "            to_val = d['value']['To']['content']\n",
    "            desc_val = d['value']['Description']['content']\n",
    "            row = {'From': from_val, 'To': to_val, 'Description': desc_val}\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            print(f\"An exception occurred while processing: {d}\")\n",
    "            print(e)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    output=os.path.join(dpath,'output')\n",
    "    df.to_csv(f'{output}\\\\Litholog - p{i}.csv', index=False)\n",
    "    i=i+1\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb8918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Files\n",
    "path_to_sample_documents = os.path.abspath(\n",
    "        os.path.join(\n",
    "            os.path.abspath(r\"C:\\Users\\test\\Data\\Lithology\"),\n",
    "            r\"Halcraw1969_PT_Lithology\\Training\\APID.pdf\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "# Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "with open(path_to_sample_documents, \"rb\") as f:\n",
    "    poller = document_analysis_client.begin_analyze_document(\n",
    "        model_id=model_id, document=f\n",
    "    )\n",
    "result = poller.result()\n",
    "# Extracting Well Details\n",
    "row = []\n",
    "data=result.documents[0].to_dict()['fields']\n",
    "for col in columns:\n",
    "    try:\n",
    "        row.append(data.get(col)['value'])\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred while processing: {d}\")\n",
    "        print(e)\n",
    "row = pd.DataFrame([row],columns=columns)\n",
    "dataset = pd.concat([dataset, row], ignore_index=True) \n",
    "dataset.to_csv(os.path.join(dpath, \"output\\WellsDataset.csv\"), index=False)\n",
    "\n",
    "#Extracting Lithology Logs\n",
    "data2=result.documents[0].to_dict()['fields']['Lithology']['value']\n",
    "rows = []\n",
    "for d in data2:\n",
    "    try:\n",
    "        from_val = d['value']['From']['content']\n",
    "        to_val = d['value']['To']['content']\n",
    "        desc_val = d['value']['Description']['content']\n",
    "        row = {'From': from_val, 'To': to_val, 'Description': desc_val}\n",
    "        rows.append(row)\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred while processing: {d}\")\n",
    "        print(e)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "output=os.path.join(dpath,'output')\n",
    "df.to_csv(f'{output}\\\\Litholog - p{i}.csv', index=False)\n",
    "i=i+1\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a3550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6387c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for idx, document in enumerate(result.documents):\n",
    "    print(\"--------Analyzing document #{}--------\".format(idx + 1))\n",
    "    print(\"Document has type {}\".format(document.doc_type))\n",
    "    print(\"Document has confidence {}\".format(document.confidence))\n",
    "    print(\"Document was analyzed by model with ID {}\".format(result.model_id))\n",
    "    for name, field in document.fields.items():\n",
    "        field_value = field.value if field.value else field.content\n",
    "        print(\"......found field of type '{}' with value '{}' and with confidence {}\".format(field.value_type, field_value, field.confidence))\n",
    "\n",
    "\n",
    "# iterate over tables, lines, and selection marks on each page\n",
    "for page in result.pages:\n",
    "    print(\"\\nLines found on page {}\".format(page.page_number))\n",
    "    for line in page.lines:\n",
    "        print(\"...Line '{}'\".format(line.content))\n",
    "    for word in page.words:\n",
    "        print(\n",
    "            \"...Word '{}' has a confidence of {}\".format(\n",
    "                word.content, word.confidence\n",
    "            )\n",
    "        )\n",
    "    for selection_mark in page.selection_marks:\n",
    "        print(\n",
    "            \"...Selection mark is '{}' and has a confidence of {}\".format(\n",
    "                selection_mark.state, selection_mark.confidence\n",
    "            )\n",
    "        )\n",
    "\n",
    "for i, table in enumerate(result.tables):\n",
    "    print(\"\\nTable {} can be found on page:\".format(i + 1))\n",
    "    for region in table.bounding_regions:\n",
    "        print(\"...{}\".format(region.page_number))\n",
    "    for cell in table.cells:\n",
    "        print(\n",
    "            \"...Cell[{}][{}] has content '{}'\".format(\n",
    "                cell.row_index, cell.column_index, cell.content\n",
    "            )\n",
    "        )\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fba8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatGPT \n",
    "for idx, document in enumerate(result.documents):\n",
    "    print(\"--------Analyzing document #{}--------\".format(idx + 1))\n",
    "    print(\"Document has type {}\".format(document.doc_type))\n",
    "    print(\"Document has confidence {}\".format(document.confidence))\n",
    "    print(\"Document was analyzed by model with ID {}\".format(result.model_id))\n",
    "    for name, field in document.fields.items():\n",
    "        field_value = field.value if field.value else field.content\n",
    "        print(\"......found field of type '{}' with value '{}' and with confidence {}\".format(field.value_type, field_value, field.confidence))\n",
    "\n",
    "    # iterate over tables, lines, and selection marks on each page\n",
    "    for page in result.pages:\n",
    "        print(\"\\n---Page {}---\".format(page.page_number))\n",
    "        print(\"\\nLines found:\")\n",
    "        for line in page.lines:\n",
    "            print(\"    - Line '{}'\".format(line.content))\n",
    "        print(\"\\nWords found:\")\n",
    "        for word in page.words:\n",
    "            print(\n",
    "                \"    - Word '{}' has a confidence of {}\".format(\n",
    "                    word.content, word.confidence\n",
    "                )\n",
    "            )\n",
    "        print(\"\\nSelection marks found:\")\n",
    "        for selection_mark in page.selection_marks:\n",
    "            print(\n",
    "                \"    - Selection mark is '{}' and has a confidence of {}\".format(\n",
    "                    selection_mark.state, selection_mark.confidence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\"\\nTables found:\")\n",
    "    for i, table in enumerate(result.tables):\n",
    "        print(\"    - Table #{}\".format(i + 1))\n",
    "        for cell in table.cells:\n",
    "            print(\n",
    "                \"        - Cell[{}][{}] has content '{}'\".format(\n",
    "                    cell.row_index, cell.column_index, cell.content\n",
    "                )\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
